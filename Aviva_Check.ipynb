{"cells":[{"cell_type":"markdown","source":["# Descripcion del ejercicio\n","\n","OBJETIVO: Predecir si el hay una pelea en un video\n","\n","El conjunto de datos contiene videos en los cuales se incliyen ecenas de pelas. Se debe de construir un clasificador binario que etiquete correctamente los videos. \n","\n","El conjunto de datos probiene de Kaggle. https://www.kaggle.com/datasets/naveenk903/movies-fight-detection-dataset\n","\n","INSTRUCCIONES.\n","\n","* Analisis de datos: Caracteristicas de los conjuntos de datos: estadísticas sobre la duración de los videos video, relacion número de frames y etiqueta, etc . \n","\n","* Seleccion del modelo: Proponer la arquitectura, los hiperparametros y el tratamiendo de los modelos para el problema de clasificacion.\n","\n","* Analisis de resultados: Inlcuir metricas como: F-measure, Precision, Recall\n","\n","* Modelo: Guardar el modelo utilizando la libreria joblib.dump"],"metadata":{"id":"xYetxzN3-Bla"}},{"cell_type":"markdown","source":["# Breve investigacion en la literatura y foros sobre problemas similares\n","\n","Para resolver ester problema en primera intancia realizo una búsqueda referentea trabajos similares. \n","\n","* Fight-Detection: Es una libreria para python que identifica peleas en tiempo real  haciendo uso del aprendizaje por transferencia en modelos 3D convolucionales previamente entrenados que tienen como objetivo reconocer los movimientos y acciones de los humanos. Todos los modelos utilizan el conjunto de datos Kinetics-400 para la parte preentrenada y el conjunto de datos de detección de peleas de cámaras de vigilancia basada en visión para la parte ajustada. Esta es la liga https://pypi.org/project/Fight-Detection/\n","\n","* Fight detection using OpenCV: Esta es una propuesta o resultado que identifica una pelea utilizando la postura humana. Clasifica las posturas e identifica si la posocion correspoende a una postura de pelea. https://www.youtube.com/watch?v=TdqeDOv3QMQ\n","\n","* Real-Time Violent Action Recognition Using Key Frames Extraction and Deep Learning. Este estudio explora la arquitectura de aprendizaje profundo de vanguardia de las redes neuronales convolucionales (CNN) y el inicio V4 para detectar y reconocer la violencia utilizando datos de video. Esta es la liga https://www.techscience.com/cmc/v69n2/43878/html\n","\n","* Violence-Detection: El método consiste en extraer un conjunto de cuadros pertenecientes al video, enviarlos a una red preentrenada, obtener la salida de una de sus capas finales y a partir de estas salidas entrenar otra arquitectura de red con un tipo de neuronas especiales llamadas LSTM. Estas neuronas tienen memoria y son capaces de analizar la información temporal del video, si en algún momento detectan violencia, será catalogado como un video violento. Esta es la liga: https://www.kaggle.com/code/yassershrief/violence-detection-hockey-fight-cnn-lstm\n"],"metadata":{"id":"FU4Rtx31CrIa"}},{"cell_type":"markdown","source":["# Pretratamiento de los datos y funciones auxiliares\n","\n","En esta seccion se desarrolla la metodología para recorrer los directorios de videos, obtener sus frames y normalizar los datos para poder procesarlos y obtener sus caracteristicas mediante una red neuronal preentrenadoa. "],"metadata":{"id":"GZ_EeKp1Z3El"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3635,"status":"ok","timestamp":1662943605683,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"},"user_tz":300},"id":"YChP8ZilM9Pp","outputId":"06fe52e3-a8d6-4ea4-9c8c-745a4ee04b68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["#Acceso al directorio al Drive donde se encuentran los videos del conjunto de datos\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1662943605684,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"},"user_tz":300},"id":"gnUsNDAjOTY4"},"outputs":[],"source":["# Defino las rutas \n","Path = '/content/gdrive/MyDrive/Entrevista/AVIVA'\n","Ruta_Peleas ='/content/gdrive/MyDrive/Entrevista/AVIVA/Peliculas/fights'\n","Ruta_NoPeleas = '/content/gdrive/MyDrive/Entrevista/AVIVA/Peliculas/noFights'"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1662943606016,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"},"user_tz":300},"id":"8ZG1COW1O1wj","outputId":"a2ee1f14-59d5-42a8-fcb1-fd1660641c91"},"outputs":[{"output_type":"stream","name":"stdout","text":["El total de videos de peleas es:  100\n","El total de videos de no peleas es:  101\n"]}],"source":["# Listo el total de videos de las carpetas\n","Files_fight = !ls -1 \"{Ruta_Peleas}\"\n","Files_no_fight = !ls -1 \"{Ruta_NoPeleas}\"\n","\n","print(\"El total de videos de peleas es: \", len(Files_fight))\n","print(\"El total de videos de no peleas es: \", len(Files_no_fight))"]},{"cell_type":"code","source":["# Parametros de entrada de las imagenes\n","img_size = 224                             # Tamaño para redimencionar los frames de los video (224x244 es el tamño de entrada de la red VGG16)\n","num_channels = 3                           # Numero de canales"],"metadata":{"id":"-EBfkCJtLUtm","executionInfo":{"status":"ok","timestamp":1662943610203,"user_tz":300,"elapsed":116,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Desarrollo una funcion que extraiga todos los frames del video y los normalice\n","import cv2\n","import numpy as np\n","\n","# Funcion para extraer los frams de un video utilizando la ruta completa del video\n","def get_frames(Vid_Ruta):\n","\n","  video = cv2.VideoCapture(Vid_Ruta)\n","  Frames = []\n","  while (video.isOpened()):\n","    ok_read, frame = video.read()\n","    if (ok_read == True):\n","      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      frame = cv2.resize(frame, dsize=(img_size, img_size))   # Redimenciono la imagen\n","      Frames.append(frame)\n","      if (cv2.waitKey(30) == ord('s')):break\n","    else: break\n","  Frames = np.array(Frames)\n","  Frames = (Frames / 255.).astype(np.float32)                 # Normalizo los datos que sean menores que 1 y mayores que 0 \n","  return Frames"],"metadata":{"id":"f7sj_lkBMl5f","executionInfo":{"status":"ok","timestamp":1662943610490,"user_tz":300,"elapsed":4,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Construyo una lista con todas las rutas de los videos y una lista con sus etiquetas\n","Video_paths = []\n","Etiquetas   = []\n","\n","for vname in Files_fight:\n","  Video_paths.append(Ruta_Peleas + '/' + vname)\n","  Etiquetas.append(np.array([0]))                        # Etiqueta 0 indica PELEA\n","\n","for vname in Files_no_fight:\n","  Video_paths.append(Ruta_NoPeleas + '/' + vname)\n","  Etiquetas.append(np.array([1]))                        # Etiqueta 1 idica No pelea\n","\n","Video_paths = np.array(Video_paths)\n","Etiquetas   = np.array(Etiquetas)"],"metadata":{"id":"0rlnIMP7R6Me","executionInfo":{"status":"ok","timestamp":1662943614479,"user_tz":300,"elapsed":130,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Obtencion de las Caracteristicas y Generacion de conjuntos para el entrenamiento\n","\n","En esta seccion se utiliza una red neuronal Preentrenada para extraer las caracteristicas utilizando los valores de su ultima capa. En este caso se utilizara la red VGG16. Otras opciones pueden ser Resnet50 o EficcientB0 por ejemplo."],"metadata":{"id":"pi1tL5qnc9HA"}},{"cell_type":"code","source":["import keras\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras import backend\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input, Dense, Activation, Dropout, LSTM\n","\n","# Importo el modelo preentrenado VGG16\n","Modelo_VGG = VGG16(include_top=True, weights='imagenet')    # Cargo la RedVGG16 con los pesos preentrenados para el conjunto de imagenet\n","\n","# efino mi modelo de extraccion de caracteristicas utilizando como salida la capa anterior a la de predicciones\n","Layer_Name = list(Modelo_VGG.layers)[-2].name\n","Features_Layer = Modelo_VGG.get_layer(Layer_Name)    \n","Features_Model = Model(inputs=Modelo_VGG.input,outputs=Features_Layer.output)\n","\n","print(\"Procesar los frames y extraer sus caristicas mediante la red VGG16 implica que: \")\n","print(\"El input de cada framde tendra una dimencion de: \", Features_Model.input_shape[1:])\n","print(\"El ouput de cada frame tendra una dimencion de: \", Features_Model.output_shape[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hnAFr7-tUvMh","executionInfo":{"status":"ok","timestamp":1662943624106,"user_tz":300,"elapsed":5336,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}},"outputId":"a46fc192-32d8-4f0e-d222-1b2478155694"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Procesar los frames y extraer sus caristicas mediante la red VGG16 implica que: \n","El input de cada framde tendra una dimencion de:  (224, 224, 3)\n","El ouput de cada frame tendra una dimencion de:  4096\n"]}]},{"cell_type":"code","source":["# Obtengo las caracteristicas o patrones asociados a cada frame del video para cada uno de los videos\n","Caracteristicas = []\n","Fallos = []\n","for i in range(len(Video_paths)):\n","  try: Caracteristicas.append(Features_Model.predict(get_frames(Video_paths[i])))\n","  except:Fallos.append(i)\n","  if i % 20 == 0: print(i)\n","\n","if (len(Fallos)>0): print(\"Hubo un fallo en los siguientes indices de video\", print(Fallos))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kpZ2E1KHtN0H","executionInfo":{"status":"ok","timestamp":1662943735972,"user_tz":300,"elapsed":111874,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}},"outputId":"7dd28b08-a4b3-4957-90a3-971de68632d7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","20\n","40\n","60\n","80\n","100\n","120\n","140\n","160\n","180\n","200\n"]}]},{"cell_type":"code","source":["# Identifico el video que contiene la mayor cantidad de frames\n","Max_Frames = 0\n","for vid in Caracteristicas:\n","  if vid.shape[0] > Max_Frames: Max_Frames = vid.shape[0]\n","\n","# Estandarizo el tamaño de todos los Frames al mayor numero de frames\n","for pos in range(len(Caracteristicas)):\n","  if Caracteristicas[pos].shape[0] <60:\n","    dif = 60 - Caracteristicas[pos].shape[0]\n","    Caracteristicas[pos] = np.append(Caracteristicas[pos],Caracteristicas[pos][:dif],axis=0)\n","\n","# NOTA: Se decidio realizar este procedimiento debido a que es preferible tener informacion repetida de los videos mas chicos en lugar de que recortar pedasos \n","# de los videos mas grandes. No se sabe en que fragmentos de los videos se encuentre la secuencia de caracteristicas que definan un video de pelea\n","\n","# Guardo el conjunto de caracteristicas obtenido procesado\n","Caracteristicas = np.array(Caracteristicas)\n","np.save(Path+'/Caracteristicas_Set.npy',Caracteristicas)\n","np.save(Path + '/Etiquetas_Set.npy',Etiquetas)"],"metadata":{"id":"f34t3Jxg13KX","executionInfo":{"status":"ok","timestamp":1662943735973,"user_tz":300,"elapsed":12,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Abro los conjuntos de datos\n","Caracteristicas = np.load(Path+'/Caracteristicas_Set.npy')\n","Etiquetas = np.load(Path + '/Etiquetas_Set.npy')"],"metadata":{"id":"B-OhVcCJQJzK","executionInfo":{"status":"ok","timestamp":1662943735973,"user_tz":300,"elapsed":12,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Divido el conjunto de datos en train, valid y test\n","# NOTA: El 60% de los datos se utilizara como entrenamiento, el 20 % para validacion y el 20 % para test\n","\n","np.random.seed(1)\n","Posiciones = np.random.permutation(len(Video_paths))                            #Barajeo las posiciones de los datos\n","Moises_Train, Moises_Valid = int(0.6*len(Etiquetas)), int(0.8*len(Etiquetas))   #Identifico los puntos donde se cortan los datos\n","\n","# Constriyo los conjuntos con las posiciones aleatorias definiadas\n","Train_X = Caracteristicas[Posiciones[:Moises_Train]]\n","Train_Y = Etiquetas[Posiciones[:Moises_Train]]\n","Valid_X = Caracteristicas[Posiciones[Moises_Train:Moises_Valid]]\n","Valid_Y = Etiquetas[Posiciones[Moises_Train:Moises_Valid]]\n","Test_X = Caracteristicas[Posiciones[Moises_Valid:]]\n","Test_Y = Etiquetas[Posiciones[Moises_Valid:]]"],"metadata":{"id":"P-Lq3LUNR5ji","executionInfo":{"status":"ok","timestamp":1662943735974,"user_tz":300,"elapsed":12,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Construyo el modelo para la clasificacion de los datos\n","\n","Para este modelo cabe destacar el uso de redes LSTM, las cuales son una buena opcion para identificar secuencias. \n"],"metadata":{"id":"RLYwKUDJUq6Z"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Esta funcion construye y entrena una red neuronal con los parametros de interes\n","def Obtain_Model(BS,LR,LSTMU,Epocas):\n","\n","  tf.random.set_seed(1)\n","  \n","  # Se construye la arquietectura propuesta para la red neuronal\n","  Clasificador = Sequential()\n","  Clasificador.add(LSTM(LSTMU, input_shape=Train_X[0].shape))\n","  Clasificador.add(Dropout(0.5))\n","  Clasificador.add(Dense(512,activation='relu'))\n","  Clasificador.add(Dropout(0.5))\n","  Clasificador.add(Dense(50,activation='relu'))\n","  Clasificador.add(Dropout(0.5))\n","  Clasificador.add(Dense(1, activation='sigmoid'))             # Funcion Sigmodea debido a que es un problema de clasificacion Binario\n","  Clasificador.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=LR),metrics=['accuracy'])\n","\n","  #Se entrena el modelo \n","  Clasificador.fit(Train_X, Train_Y,\n","                   epochs=Epocas,\n","                   validation_data=(Valid_X, Valid_Y), \n","                   batch_size=BS,\n","                  verbose=0)\n","  \n","  # Se regresa el modelo entrenado\n","  return Clasificador"],"metadata":{"id":"d1_LBZcjUpyg","executionInfo":{"status":"ok","timestamp":1662943735976,"user_tz":300,"elapsed":14,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Propongo algunos valores a los parametros mas importantes para el diseño y entrenamiento del modelo de clasificacion \n","Ba_Sizes = [8,16,32]             # Tamaños propuestos para el Batch size\n","LSTM_Un  = [256,512,1024]        # Numero de unidades en la primer capa LSTM\n","Le_Reates = [1e-3,1e-4,1e-5]     # Learning Rates propuestos para el entrenamiento\n","Epocas = 100  \n","\n","# Obtengo todas las combinaciones de valores \n","import itertools \n","Combinaciones = list(itertools.product(Ba_Sizes,Le_Reates,LSTM_Un))\n","\n","# Para cada una de las combinaciones (GRIDSEARCH) identifico el mejor modelo\n","Best_Comb = Combinaciones[0]\n","\n","Scores = []\n","for comb in Combinaciones:\n","  Clasificador = Obtain_Model(comb[0],comb[1],comb[2],Epocas)  # BS,LR,LSTMU,Epocas\n","  Score_Valid = Clasificador.evaluate(Valid_X,Valid_Y)\n","  Score_Test  = Clasificador.evaluate(Test_X,Test_Y)\n","  Scores.append(np.array(  [float(Score_Valid[1]),float(Score_Test[1])]   ))\n","  print(\"ScoreValid: \",float(Score_Valid[1]), \"ScoreTest: \",float(Score_Test[1]), comb)\n","\n","Scores = np.array(Scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGQs7qbN2_vA","executionInfo":{"status":"ok","timestamp":1662944854805,"user_tz":300,"elapsed":359374,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}},"outputId":"8e0d2147-2b78-43be-ce92-57f5dcadf26c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 0s 15ms/step - loss: 0.6923 - accuracy: 0.5250\n","2/2 [==============================] - 0s 15ms/step - loss: 0.6950 - accuracy: 0.4634\n","ScoreValid:  0.5249999761581421 ScoreTest:  0.46341463923454285 (8, 0.001, 256)\n","2/2 [==============================] - 0s 17ms/step - loss: 0.6932 - accuracy: 0.4750\n","2/2 [==============================] - 0s 21ms/step - loss: 0.6931 - accuracy: 0.5366\n","ScoreValid:  0.4749999940395355 ScoreTest:  0.5365853905677795 (8, 0.001, 512)\n","2/2 [==============================] - 0s 22ms/step - loss: 0.6931 - accuracy: 0.5250\n","2/2 [==============================] - 0s 24ms/step - loss: 0.6932 - accuracy: 0.4634\n","ScoreValid:  0.5249999761581421 ScoreTest:  0.46341463923454285 (8, 0.001, 1024)\n","2/2 [==============================] - 0s 13ms/step - loss: 0.1347 - accuracy: 0.9750\n","2/2 [==============================] - 0s 13ms/step - loss: 0.1173 - accuracy: 0.9756\n","ScoreValid:  0.9750000238418579 ScoreTest:  0.9756097793579102 (8, 0.0001, 256)\n","2/2 [==============================] - 0s 18ms/step - loss: 0.1416 - accuracy: 0.9500\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0314 - accuracy: 0.9756\n","ScoreValid:  0.949999988079071 ScoreTest:  0.9756097793579102 (8, 0.0001, 512)\n","2/2 [==============================] - 1s 28ms/step - loss: 0.0737 - accuracy: 0.9500\n","2/2 [==============================] - 0s 33ms/step - loss: 0.0266 - accuracy: 1.0000\n","ScoreValid:  0.949999988079071 ScoreTest:  1.0 (8, 0.0001, 1024)\n","2/2 [==============================] - 0s 14ms/step - loss: 0.1315 - accuracy: 0.9750\n","2/2 [==============================] - 0s 15ms/step - loss: 0.0034 - accuracy: 1.0000\n","ScoreValid:  0.9750000238418579 ScoreTest:  1.0 (8, 1e-05, 256)\n","2/2 [==============================] - 0s 18ms/step - loss: 0.2068 - accuracy: 0.9750\n","2/2 [==============================] - 0s 22ms/step - loss: 0.2025 - accuracy: 0.9756\n","ScoreValid:  0.9750000238418579 ScoreTest:  0.9756097793579102 (8, 1e-05, 512)\n","2/2 [==============================] - 0s 25ms/step - loss: 0.2436 - accuracy: 0.9750\n","2/2 [==============================] - 0s 22ms/step - loss: 7.3365e-05 - accuracy: 1.0000\n","ScoreValid:  0.9750000238418579 ScoreTest:  1.0 (8, 1e-05, 1024)\n","2/2 [==============================] - 0s 14ms/step - loss: 0.1712 - accuracy: 0.9500\n","2/2 [==============================] - 0s 13ms/step - loss: 0.1451 - accuracy: 0.9512\n","ScoreValid:  0.949999988079071 ScoreTest:  0.9512194991111755 (16, 0.001, 256)\n","2/2 [==============================] - 0s 15ms/step - loss: 0.1559 - accuracy: 0.9500\n","2/2 [==============================] - 0s 14ms/step - loss: 0.1724 - accuracy: 0.9512\n","ScoreValid:  0.949999988079071 ScoreTest:  0.9512194991111755 (16, 0.001, 512)\n","2/2 [==============================] - 0s 21ms/step - loss: 0.6930 - accuracy: 0.5250\n","2/2 [==============================] - 0s 23ms/step - loss: 0.6933 - accuracy: 0.4634\n","ScoreValid:  0.5249999761581421 ScoreTest:  0.46341463923454285 (16, 0.001, 1024)\n","2/2 [==============================] - 0s 13ms/step - loss: 0.0843 - accuracy: 0.9750\n","2/2 [==============================] - 0s 14ms/step - loss: 0.0354 - accuracy: 0.9756\n","ScoreValid:  0.9750000238418579 ScoreTest:  0.9756097793579102 (16, 0.0001, 256)\n","2/2 [==============================] - 0s 18ms/step - loss: 0.1030 - accuracy: 0.9500\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0396 - accuracy: 0.9756\n","ScoreValid:  0.949999988079071 ScoreTest:  0.9756097793579102 (16, 0.0001, 512)\n","2/2 [==============================] - 0s 30ms/step - loss: 0.1554 - accuracy: 0.9750\n","2/2 [==============================] - 0s 30ms/step - loss: 0.1363 - accuracy: 0.9756\n","ScoreValid:  0.9750000238418579 ScoreTest:  0.9756097793579102 (16, 0.0001, 1024)\n","2/2 [==============================] - 0s 13ms/step - loss: 0.1181 - accuracy: 0.9750\n","2/2 [==============================] - 0s 16ms/step - loss: 0.0100 - accuracy: 1.0000\n","ScoreValid:  0.9750000238418579 ScoreTest:  1.0 (16, 1e-05, 256)\n","2/2 [==============================] - 0s 18ms/step - loss: 0.3266 - accuracy: 0.9500\n","2/2 [==============================] - 0s 22ms/step - loss: 0.1747 - accuracy: 0.9756\n","ScoreValid:  0.949999988079071 ScoreTest:  0.9756097793579102 (16, 1e-05, 512)\n","2/2 [==============================] - 0s 24ms/step - loss: 0.2037 - accuracy: 0.9750\n","2/2 [==============================] - 0s 24ms/step - loss: 2.8143e-04 - accuracy: 1.0000\n","ScoreValid:  0.9750000238418579 ScoreTest:  1.0 (16, 1e-05, 1024)\n","2/2 [==============================] - 0s 15ms/step - loss: 0.1663 - accuracy: 0.9500\n","2/2 [==============================] - 0s 16ms/step - loss: 0.1542 - accuracy: 0.9756\n","ScoreValid:  0.949999988079071 ScoreTest:  0.9756097793579102 (32, 0.001, 256)\n","2/2 [==============================] - 0s 20ms/step - loss: 0.2766 - accuracy: 0.9500\n","2/2 [==============================] - 0s 20ms/step - loss: 0.1877 - accuracy: 0.9756\n","ScoreValid:  0.949999988079071 ScoreTest:  0.9756097793579102 (32, 0.001, 512)\n","2/2 [==============================] - 0s 23ms/step - loss: 0.1781 - accuracy: 0.9500\n","2/2 [==============================] - 0s 25ms/step - loss: 0.1116 - accuracy: 0.9756\n","ScoreValid:  0.949999988079071 ScoreTest:  0.9756097793579102 (32, 0.001, 1024)\n","2/2 [==============================] - 0s 15ms/step - loss: 0.1820 - accuracy: 0.9500\n","2/2 [==============================] - 0s 17ms/step - loss: 0.0888 - accuracy: 0.9756\n","ScoreValid:  0.949999988079071 ScoreTest:  0.9756097793579102 (32, 0.0001, 256)\n","2/2 [==============================] - 0s 16ms/step - loss: 0.1081 - accuracy: 0.9750\n","2/2 [==============================] - 0s 16ms/step - loss: 0.0865 - accuracy: 0.9756\n","ScoreValid:  0.9750000238418579 ScoreTest:  0.9756097793579102 (32, 0.0001, 512)\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0664 - accuracy: 0.9750\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0508 - accuracy: 0.9756\n","ScoreValid:  0.9750000238418579 ScoreTest:  0.9756097793579102 (32, 0.0001, 1024)\n","2/2 [==============================] - 0s 18ms/step - loss: 0.1141 - accuracy: 0.9750\n","2/2 [==============================] - 0s 14ms/step - loss: 0.0229 - accuracy: 1.0000\n","ScoreValid:  0.9750000238418579 ScoreTest:  1.0 (32, 1e-05, 256)\n","2/2 [==============================] - 0s 16ms/step - loss: 0.2504 - accuracy: 0.9500\n","2/2 [==============================] - 0s 18ms/step - loss: 0.1506 - accuracy: 0.9756\n","ScoreValid:  0.949999988079071 ScoreTest:  0.9756097793579102 (32, 1e-05, 512)\n","2/2 [==============================] - 0s 23ms/step - loss: 0.1952 - accuracy: 0.9750\n","2/2 [==============================] - 0s 23ms/step - loss: 5.9457e-04 - accuracy: 1.0000\n","ScoreValid:  0.9750000238418579 ScoreTest:  1.0 (32, 1e-05, 1024)\n"]}]},{"cell_type":"code","source":["# Identifico la combinacion con la cual se obtuvieron los mejores resultados en la validacion y el test\n","Pos_Best_Com = np.argmax(Scores.sum(axis=1))\n","Best_Com = Combinaciones[Pos_Best_Com]\n","\n","# Re entreno el modelo con estas combinaciones y lo guardo\n","Clasificador = Obtain_Model(Best_Com[0],Best_Com[1],Best_Com[2],Epocas)\n","Clasificador.save(Path + '/Clasificador_Pelea.h5')\n","\n","import joblib\n","joblib.dump(Clasificador , Path + '/Clasificador_jlib')"],"metadata":{"id":"zxuIm0DKGWVb","executionInfo":{"status":"ok","timestamp":1662945122451,"user_tz":300,"elapsed":24169,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Revision de Resultados\n","\n","A continuacion se revisan los resultados obtenidos por el modelo de clasifiacion realizado"],"metadata":{"id":"xOlpVh9LRYTb"}},{"cell_type":"code","source":["print(\"Los parametros para el mejor modelo son: \")\n","print(\"Batch Size: \", Best_Com[0])\n","print(\"Learning Rate: \", Best_Com[1])\n","print(\"Unidades LSTM: \", Best_Com[2])\n","print(\"El rendimiento en el conjunto de Validacione es de: \", Scores[Pos_Best_Com][1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XYh3o9xVQT9s","executionInfo":{"status":"ok","timestamp":1662945318051,"user_tz":300,"elapsed":140,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}},"outputId":"7d305e4a-37d9-408d-aa5f-2f5a1caa0acb"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Los parametros para el mejor modelo son: \n","Batch Size:  8\n","Learning Rate:  1e-05\n","Unidades LSTM:  256\n","El rendimiento en el conjunto de Validacione es de:  1.0\n"]}]},{"cell_type":"code","source":["# Calculo y grafico la matriz de confuncion\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Proceso los datos de test reales y predecidps\n","Predecidos = Clasificador.predict(Test_X)\n","Predecidos = Predecidos.reshape(-1)\n","Pos_Zeros  = np.where(Predecidos<0.5)[0]\n","Pred_Test = np.ones(len(Predecidos))\n","Pred_Test[Pos_Zeros] = 0\n","Reales = Test_Y.reshape(-1)\n","\n","# Calculo la matriz de confusion\n","Res_CM = confusion_matrix(Pred_Test,Reales)\n","\n","# Grafico la matriz de confucion\n","dataframe = pd.DataFrame(Res_CM, index=['Pelea', 'No Pelea'], columns=['Pelea', 'No Pelea'])\n","sns.heatmap(dataframe, annot=True, cbar=None, cmap=\"Blues\")\n","plt.title(\"Confusion Matrix\"), plt.tight_layout()\n","plt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"id":"6FY2Xr2cRDbN","executionInfo":{"status":"ok","timestamp":1662947292600,"user_tz":300,"elapsed":393,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}},"outputId":"43949d24-9dfc-4bdb-f0fc-6e5cbbcd7860"},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAbYAAAEmCAYAAAAOb7UzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaMElEQVR4nO3dd5wlZZ3v8c8XBhhkyKFBSSKGJSyIiwiuCLoqQWUUFFdMCKK4oC6m3atihGvC66K4iiAqIKLXBIKAF0SCgTgwgICBIAsMSUGSTPjdP07N0NN29/T0dPUZaz7v16tfnKpTVc/vNNP97afqqadSVUiS1BXL9bsASZImksEmSeoUg02S1CkGmySpUww2SVKnGGySpE4x2KQlkGTlJKcnuT/Jd5fgOPslOWcia+uHJD9J8sZ+16Flm8GmZUKS1ya5LMmDSe5ofgH/8wQceh9gAFi7ql413oNU1clV9eIJqGchSXZJUkl+MGT9Ns3688d4nI8kOWlR21XV7lX1jXGWK00Ig02dl+Qw4PPAkfRCaGPgS8BeE3D4TYAbq2rOBByrLXcDOyZZe9C6NwI3TlQD6fH3iZYK/kNUpyVZHfgY8G9V9f2qeqiqZlfV6VX13mablZJ8Psntzdfnk6zUvLdLktuSvDvJXU1vb//mvY8ChwP7Nj3BA4b2bJJs2vSMpjTLb0ryhyR/SXJTkv0Grb9o0H47Jbm0OcV5aZKdBr13fpKPJ7m4Oc45SdYZ5dvwGPBD4DXN/ssD+wInD/le/VeSPyZ5IMnlSZ7XrN8N+F+DPudVg+o4IsnFwMPAZs26A5v3/zvJ9wYd/1NJzk2SMf8PlMbBYFPX7QhMBX4wyjYfAJ4DbAtsAzwb+OCg99cHVgeeBBwAHJNkzar6ML1e4KlVNa2qjh+tkCSrAEcDu1fVqsBOwIxhtlsLOKPZdm3gc8AZQ3pcrwX2B9YDVgTeM1rbwDeBNzSvXwJcA9w+ZJtL6X0P1gK+BXw3ydSqOmvI59xm0D6vBw4CVgVuGXK8dwNbN6H9PHrfuzeW8/ipZQabum5t4J5FnCrcD/hYVd1VVXcDH6X3C3u+2c37s6vqTOBB4OnjrGcesFWSlavqjqq6dpht9gR+W1UnVtWcqjoFuB542aBtTqiqG6vqEeA79AJpRFX1C2CtJE+nF3DfHGabk6rq3qbNo4CVWPTn/HpVXdvsM3vI8R6m9338HHAScGhV3baI40lLzGBT190LrDP/VOAInsjCvY1bmnULjjEkGB8Gpi1uIVX1EL1TgG8D7khyRpJnjKGe+TU9adDyneOo50TgEGBXhunBJnlPkt80pz//TK+XOtopToA/jvZmVf0a+AMQegEstc5gU9f9EvgrMH2UbW6nNwhkvo3529N0Y/UQ8IRBy+sPfrOqzq6qFwEb0OuFfXUM9cyv6X/GWdN8JwJvB85selMLNKcK3we8GlizqtYA7qcXSAAjnT4c9bRikn+j1/O7vTm+1DqDTZ1WVffTG+BxTJLpSZ6QZIUkuyf5dLPZKcAHk6zbDMI4nN6ps/GYAeycZONm4Mp/zn8jyUCSvZprbX+ld0pz3jDHOBN4WnOLwpQk+wJbAD8eZ00AVNVNwPPpXVMcalVgDr0RlFOSHA6sNuj9WcCmizPyMcnTgE8Ar6N3SvJ9SUY9ZSpNBINNnddcLzqM3oCQu+mdPjuE3khB6P3yvQy4GpgJXNGsG09bPwVObY51OQuH0XJNHbcD99ELmYOHOca9wEvpDb64l15P56VVdc94ahpy7Iuqarje6NnAWfRuAbgFeJSFTzPOv/n83iRXLKqd5tTvScCnquqqqvotvZGVJ84fcSq1JQ5QkiR1iT02SVKnGGySpE4x2CRJnWKwSZI6ZbSbVvtqlX1OcFSL1Lj32/v3uwRpqTN1CsPOO2qPTZLUKQabJKlTDDZJUqcYbJKkTjHYJEmdYrBJkjrFYJMkdYrBJknqFINNktQpBpskqVMMNklSpxhskqROMdgkSZ1isEmSOsVgkyR1isEmSeoUg02S1CkGmySpUww2SVKnGGySpE4x2CRJnWKwSZI6xWCTJHWKwSZJ6hSDTZLUKQabJKlTDDZJUqcYbJKkTjHYJEmdYrBJkjrFYJMkdYrBJknqFINNktQpBpskqVMMNklSpxhskqROMdgkSZ1isEmSOsVgkyR1isEmSeoUg02S1CkGmySpUww2SVKnGGySpE4x2CRJnWKwSZI6xWCTJHWKwSZJ6pRWgy3Jc5JcmuTBJI8lmZvkgTbblCQt29rusX0R+Ffgt8DKwIHAMS23KUlahrV+KrKqfgcsX1Vzq+oEYLe225QkLbumtHz8h5OsCMxI8mngDryuJ0lqUdsh8/qmjUOAh4CNgL1bblOStAxrtcdWVbckWRnYoKo+2mZbkiRB+6MiXwbMAM5qlrdNclqbbWr8/vvtz+Xm41/DpZ+bvmDd1pusyXlH7MklR03nu//xQlZdeYU+Vij118UXXsDL93wJL93tRRz/1WP7XY5G0PapyI8Azwb+DFBVM4Ant9ymxumkn/2O6Z/46ULrjjn4uRx+8mU8+90/5PRLbuVde23Vp+qk/po7dy5HHvExvvTl4/jBaWdw1pk/5ve/+12/y9Iw2g622VV1/5B11XKbGqeLfzOL+x7860LrNt9gdS66bhYA5151O3vtsGkfKpP675qZV7PRRpuw4UYbscKKK7LbHnty/s/O7XdZGkbbwXZtktcCyyd5apIvAL9ouU1NoN/c9mdeuv3GALxyx03ZcJ1V+lyR1B93zZrF+husv2B5vYEBZs2a1ceKNJK2g+1QYEvgr8ApwAPAu0baOMlBSS5LctmcP5zfcmkai4OPuYiDdnsGF33qZUxbeQUemzO33yVJ0qjaHhX5MPCB5mss2x8LHAuwyj4neMpyKXDj7ffz8o+fA8DmG6zGbttt2OeKpP5Yb2CAO++4c8HyXbNmMTAw0MeKNJJWgi3J6YxyLa2qXt5Gu5p46642lbsfeJQE3r/PNhz/0xv6XZLUF1tutTW33nozt932RwbWG+CsM8/gf3/mqH6XpWG01WP7bEvHVYu+/q7n87wt12ftVady41dezSdOvZJpU1fgoN2eAcBpv76Fb5732z5XKfXHlClT+M8PHM7BBx3IvHlzmf6Kvdl886f2uywNI1XtnvFrbtDeuKoW6099T0VKj7v32/v3uwRpqTN1ChluvTdoS5I6xRu0JUmd4g3akqROafuxNQvdoA28A2/QliS1aDJv0P4WcD+j3KAtSdKSaus+tqnA24DNgZnAjlU1p422JEkarK0e2zeAf6IXarvjfW2SpEnS1jW2Lapqa4AkxwOXtNSOJEkLaavHNnv+C09BSpImU1s9tm2SPNC8DrBysxygqmq1ltqVJC3jWgm2qlq+jeNKkrQobQ/3lyRpUhlskqROMdgkSZ1isEmSOsVgkyR1isEmSeoUg02S1CkGmySpUww2SVKnGGySpE4x2CRJnWKwSZI6xWCTJHWKwSZJ6hSDTZLUKQabJKlTDDZJUqcYbJKkTlmsYEuyXJLV2ipGkqQltchgS/KtJKslWQW4BrguyXvbL02SpMU3lh7bFlX1ADAd+AnwZOD1rVYlSdI4jSXYVkiyAr1gO62qZgPVblmSJI3PWILtK8DNwCrABUk2AR5osyhJksZryqI2qKqjgaMHrbolya7tlSRJ0viNZfDIO5vBI0lyfJIrgBdMQm2SJC22sZyKfHMzeOTFwJr0Bo58stWqJEkap7EEW5r/7gGcWFXXDlonSdJSZSzBdnmSc+gF29lJVgXmtVuWJEnjs8jBI8ABwLbAH6rq4SRrA/u3W5YkSeMzllGR85LcBDwtydRJqEmSpHFbZLAlORB4J7AhMAN4DvBLHBkpSVoKjeUa2zuB7YFbqmpX4JnAn1utSpKkcRpLsD1aVY8CJFmpqq4Hnt5uWZIkjc9YBo/clmQN4IfAT5P8Cbil3bIkSRqfsQweeUXz8iNJfgasDpzValWSJI3TiMGWZK1hVs9s/jsNuK+ViiRJWgKj9dgup/d4msGzjMxfLmCzFuuSJGlcRgy2qnryZBYiSdJEGHFUZJKXJNlnmPV7J3lRu2VJkjQ+ow33Pxz4+TDrfw58rJ1yJElaMqMF20pVdffQlVV1D72naUuStNQZLdhWS/I31+CSrACs3F5JkiSNX6pq+DeSTwIDwCFV9VCzbhrwX8A9VfX+Ngt7dA7DFyYtg9bc/pB+lyAtdR658ovDPht0tB7bB4FZwC1JLk9yOXATcHfzniRJS53RhvvPAf4jyUeBzZvVv6uqRyalMkmSxmEsU2o9wuMzjkiStFQby+z+kiT93TDYJEmdsshgS8/rkhzeLG+c5NntlyZJ0uIbS4/tS8COwL82y38BjmmtIkmSlsBYHjS6Q1Vtl+RKgKr6U5IVW65LkqRxGUuPbXaS5ek9qoYk6wLzWq1KkqRxGkuwHQ38AFgvyRHARcCRrVYlSdI4jeU+tpObWUdeSO8ho9Or6jetVyZJ0jgsMtiSbAw8DJw+eF1V3dpmYZIkjcdYBo+cQe/6WoCpwJOBG4AtW6xLkqRxGcupyK0HLyfZDnh7axVJkrQEFnvmkaq6AtihhVokSVpiY7nGdtigxeWA7YDbW6tIkqQlMJZrbKsOej2H3jW377VTjiRJS2bUYGtuzF61qt4zSfVIkrRERrzGlmRKVc0FnjuJ9UiStERG67FdQu962owkpwHfBR6a/2ZVfb/l2iRJWmxjucY2FbgXeAGP389WgMEmSVrqjBZs6zUjIq/h8UCbr1qtSpKkcRot2JYHprFwoM1nsEmSlkqjBdsdVfWxSatEkqQJMNrMI8P11CRJWqqNFmwvnLQqJEmaICMGW1XdN5mFSJI0ERZ7EmRJkpZmBpskqVMMNklSpxhskqROMdgkSZ1isEmSOsVgkyR1isEmSeoUg02S1CkGmySpUww2SVKnjOUJ2uOSZCpwALAlvadwA1BVb26rTUmS2uyxnQisD7wE+DmwIfCXFtuTJKnVYNu8qj4EPFRV3wD2BHZosT1JkloNttnNf/+cZCtgdWC9FtuTJKm9a2zAsUnWBD4EnAZMAw5vsT1JktoLtqo6rnn5c2CzttqRJGmw1k5FJhlIcnySnzTLWyQ5oK32JEmCdq+xfR04G3his3wj8K4W25MkqdVgW6eqvgPMA6iqOcDcFtuTJKnVYHsoydpAASR5DnB/i+1JktTqqMjD6I2GfEqSi4F1gX1abE+SpFZHRV6R5PnA04EAN1TV7EXsJknSEpnwYEvyyhHeeloSqur7E92mJEnztdFje9ko7xVgsEmSWjPhwVZV+0/0MSVJGqs2H1szABwJPLGqdk+yBbBjVR3fVpuaWBdfeAGf+uQRzJs7j1fs/SoOeMtB/S5JmlQbDqzBcR9/A+utvSpV8LXvXcwxp5zPke+azh47b8Vjs+dy0233cNCHT+L+Bx/pd7lqpKraOXBvxpETgA9U1TZJpgBXVtXWY9n/0Tm0U5jGZO7cubx8z5fwla+ewMDAAK/ddx8++ZnP8ZTNN+93acukNbc/pN8lLJPWX2c11l9nNWZcfxvTnrASv/jW+3n1YcfypPXW4PxLb2Tu3Hl84h17AfDBo3/U52qXPY9c+cUMt94btDWsa2ZezUYbbcKGG23ECiuuyG577Mn5Pzu332VJk+rOex5gxvW3AfDgw3/l+pvu5InrrsG5v7qeuXPnAXDJzJt40sAa/SxTQ3iDtoZ116xZrL/B+guW1xsYYNasWX2sSOqvjTdYi22fviGXXnPzQuvfsNeOnH3xdf0pSsNqM9iG3qD9TeDQ0XZIclCSy5JcdvxXj22xNEkau1VWXpFTPnsg7/3s9/jLQ48uWP++A17C3Lnz+PaZl/axOg3VyuCRJOvSC83p9B4uOqYbtKvqWOBY8Bpbv603MMCdd9y5YPmuWbMYGBjoY0VSf0yZshynfPYtnPqTy/jReVctWP+6l+3AHjtvxe5vPbqP1Wk4E95jS3IgcC3wBWAm8JSqusZZR/6+bLnV1tx6683cdtsfmf3YY5x15hk8f9cX9LssadJ9+cP7ccNNd3L0SectWPeinf6Bw970L+zzrq/wyKP+alvaTPioyCTXALtW1d1JNgNOrqodF/c49tj678ILfs6nP3kk8+bNZfor9uYtbz243yUtsxwV2R87bbsZ555wGDNv/B/mNb8rP/zF0zjqva9ipRWncO/9DwFwycybeccR3+5nqcukkUZFthFsV1TVdiMtj5XBJj3OYJP+1kjB1sY1tg2THD3SclW9o4U2JUkC2gm29w5ZvryFNiRJGlYbc0V+Y6KPKUnSWLV5H5skSZPOYJMkdYrBJknqlNaCLcmGSX6Q5O4kdyX5XpIN22pPkiRot8d2Ar25IjcAngic3qyTJKk1bQbbulV1QlXNab6+DqzbYnuSJLUabPcmeV2S5Zuv1wH3ttieJEmtBtubgVcDdwJ3APsA+7fYniRJ7Ty2BqCqbgFe3tbxJUkazoQHW5LDR3m7qurjE92mJEnztdFje2iYdasABwBrAwabJKk1bcwVedT810lWBd5J79rat4GjRtpPkqSJ0Mo1tiRrAYcB+wHfALarqj+10ZYkSYO1cY3tM8ArgWOBravqwYluQ5KkkbQx3P/d9GYa+SBwe5IHmq+/JHmghfYkSVqgjWtsTqwsSeobQ0iS1CkGmySpUww2SVKnGGySpE4x2CRJnWKwSZI6xWCTJHWKwSZJ6hSDTZLUKQabJKlTDDZJUqcYbJKkTjHYJEmdYrBJkjrFYJMkdYrBJknqFINNktQpBpskqVMMNklSpxhskqROMdgkSZ1isEmSOsVgkyR1isEmSeoUg02S1CkGmySpUww2SVKnGGySpE4x2CRJnWKwSZI6xWCTJHWKwSZJ6hSDTZLUKQabJKlTDDZJUqekqvpdg5ZySQ6qqmP7XYe0NPDnYelnj01jcVC/C5CWIv48LOUMNklSpxhskqROMdg0Fl5PkB7nz8NSzsEjkqROsccmSeoUg02S1CkG2zImydwkM5Jck+S7SZ4wyra7JPnxZNYnTaQkleSoQcvvSfKRxdj/TUnubn5mrkvylkVs//Uk+yxByZoABtuy55Gq2raqtgIeA97W74KkFv0VeGWSdZbgGKdW1bbALsCRSQYmpDK1xmBbtl0IbJ5klSRfS3JJkiuT7DV0w5G2SbJpkguTXNF87TTpn0Ia2Rx6oxj/fegbzb/d85JcneTcJBuPdqCqugv4PbBJkmcl+XmSy5OcnWSDYY4/7DZJ3pLk0iRXJfneaGdNND4G2zIqyRRgd2Am8AHgvKp6NrAr8JkkqwzZZaRt7gJeVFXbAfsCR0/WZ5DG6BhgvySrD1n/BeAbVfWPwMks4t9uks2AzYBbmn33qapnAV8Djhiy7QqjbPP9qtq+qrYBfgMcsCQfTn9rSr8L0KRbOcmM5vWFwPHAL4CXJ3lPs34qMPSv1xePsM3twBeTbAvMBZ7WZvHS4qqqB5J8E3gH8Migt3YEXtm8PhH49AiH2DfJP9M7rflWYF1gK+CnSQCWB+4Yss/TR9lmqySfANYApgFnj/vDaVgG27LnkeZ6wQLp/eTtXVU3DFk/+FrCSNt8BJgFbEPvDMCjbRQtLaHPA1cAJ4xj31Or6pD5C0m2Bq6tqh1H2SejbPN1YHpVXZXkTfSu3WkCeSpS0PuL8dAm4EjyzMXYZnXgjqqaB7ye3l+m0lKlqu4DvsPCp/1+Abymeb0fvTMYY3EDsG6SHaF32jHJlouxzarAHc3pyv0W+8NokQw2AXwcWAG4Osm1zfJYt/kS8MYkVwHPAB6ahHql8TgKGDw68lBg/yRX0/uj7J1jOUhVPQbsA3yq+Xc/A9hpMbb5EPBr4GLg+nF/Go3IKbUkSZ1ij02S1CkGmySpUww2SVKnGGySpE4x2CRJnWKwScNYnKcgjOFYC2Z8T3Jcki1G2XaX8cy3meTm4Sb6TTItyVeS/L6Zs/D8JDs07z24uO1Ifw8MNml4oz4FoZlrc7FV1YFVdd0om+zCkHuiltBxwH3AU5s5C/dn4Xu5pM4x2KRFm/8UhF2aJxmcBlyXZPkkn2lmar86yVuhN0VZki8muSHJ/wPWm3+gpsf0T83r3ZonIlzVzC6/Kb0A/femt/i8JOs2M8Bf2nw9t9l37STnJLk2yXH0pnBaSJKnADsAH2xmhqGqbqqqM4ZsN61p/4okMwc9uWGVJGc09V2TZN9m/SfTezbZ1Uk+O7HfamnJOVekNIpBT0E4q1m1HbBVVd2U5CDg/qraPslKwMVJzgGeSW8S3C2AAeA6erO7Dz7uusBXgZ2bY61VVfcl+TLwYFV9ttnuW8D/qaqL0nusytnAPwAfBi6qqo8l2ZPhZ4jfEphRVXMX8TEfBV7RTBa8DvCrJrx3A26vqj2bWlZPsjbwCuAZVVVJ1hjbd1KaPAabNLzhnoKwE3BJVd3UrH8x8I95/InJqwNPBXYGTmkC5fYk5w1z/OcAF8w/VjOX4XD+BdiimaITYLUk05o2Xtnse0aSP43zc0Kvt3dkkp2BecCT6AXyTOCoJJ8CflxVFzZB/yhwfHpPV/cJ61rqGGzS8IZ7CgIsPBdmgEOr6uwh2+0xgXUsBzynqhZ6asKgoBvNtcA2SZZfRK9tP3qPYnlWVc1OcjMwtapuTLIdsAfwiSTnNj3EZwMvpDcX4iHACxb7U0kt8hqbNH5nAwc3s7ST5GnpPXz1AnrP8Fo+vacm7zrMvr8Cdk7y5GbftZr1f6E3+/t859CbrJdmu/lhewHw2mbd7sCaQxuoqt8DlwEfTRY8lWHT5tTlYKsDdzWhtiuwSbPtE4GHq+ok4DPAdk1vcfWqOpPeU6m3WdQ3SZps9tik8TsO2BS4ogmOu4HpwA/o9WKuA24Ffjl0x6q6u7lG9/0ky9E8iRw4Hfi/zQCOQ+k9HPOYZgb6KfQC7W3AR4FT0nvSwi+adoZzIL1Z7X+X5BHgHuC9Q7Y5GTg9yUx6QTh/xvmt6T0pfR4wGziYXuj+KMlUej3Ww8b2rZImj7P7S5I6xVORkqROMdgkSZ1isEmSOsVgkyR1isEmSeoUg02S1CkGmySpU/4/XHf0XeJRbDEAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# Metricas de medicion\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","\n","Precision = precision_score(Pred_Test,Reales)\n","Recall   = recall_score(Pred_Test,Reales)\n","F1        = f1_score(Pred_Test,Reales)\n","\n","print(\"A continuacion se introducen algunas metricas para analizar el comportamiento de clasificador\")\n","print(\"Precision Score: \", Precision)\n","print(\"Recall Score: \", Recall)\n","print(\"F1 Score\", F1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yYl90pW3ZEC1","executionInfo":{"status":"ok","timestamp":1662948207900,"user_tz":300,"elapsed":2432,"user":{"displayName":"Nicolas Cortes","userId":"07182427027838422338"}},"outputId":"7f617068-be90-4ce4-f0fe-81e45ee786b6"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["A continuacion se introducen algunas metricas para analizar el comportamiento de clasificador\n","Precision Score:  1.0\n","Recall Score:  1.0\n","F1 Score 1.0\n"]}]},{"cell_type":"markdown","source":["# Importancia de las caracteristicas\n","\n","Aunado al modelo de clasificacion es posible generar un modelo que identifique las caracteristicas más importantes de los frames. Los resultados serian muy similares a los que se presentan en este video tutorial. https://www.youtube.com/watch?v=dw63QH_b3Jo. Para este problema de clasificacion, implementar este modelo nos ayudaria a identificar cuales son las caracteristicas o las secuencias sobre las que se basa el clasificador para identificar una pela. "],"metadata":{"id":"RvKPCahldCKY"}},{"cell_type":"markdown","source":["# Comentarios\n","\n","Con base en los resultados obtenidos y a lo revisado en la investigación previa se resaltan las siguientes obervaciones:\n","\n","* PRECISIÓN DEL MODELO: Se observó que el modelo clasifico exitosamente a todos los elementos del conjunto de Test. Esto puede deberse a que muchos videos son fragmentos de una misma pelicula. Cabe la posibilidad de que se aprendiera a identificar estos ecenarios en lugar de las características que definen a una pelea. Resulta importante revisar este punto para dar confiabilidad al modelo.  \n","* EXTENSIÓN DEL CONJUNTO DE VIDEOS: Es importante notar que el entrenamiento y la validacion se realizo utilizando un conjunto de modelos bastante rudido. Para extender la generalidad del modelo es necesario utilizar un conjunto de videos más diverso y extenso. \n","* OTRAS REDES CNN: En caso de no obtener buenos resultados con la arquitectura planteada, se podrian revisar otras redes preentrenadas. Dentro de ellas destaco la posibilidad de usar alguna de la famila EFICCIENT, ya que han mostrado tener un muy buen desempeño en diferentes problemas.\n","* OTRAS REDES RECURRENTES: En caso de no tener buenos resultados con la arquitectura planteada, propondria utilizar una TCN para resolver este problema, ya que como su nombre lo indica, es una red convulucional temporal, la cual podria encontrar de mejor manera una solucion al problema. \n","* DATA AUGMENTION: Debido a las caracteristicas del problema, es posible realizar aumentar el número de los videos aplicando operaciones a cada uno de los frames de los videos. De esta manera podriamos obtener un dataset mas nutrido para la extración y clasificación de características y secuencias. Esta propuesta tiene la desventaja de que también requere un uso más intensivo de recursos. \n","* OTRO ENFOQUE: Otro enfoque para resolver este problema podría ser el uso de YOLO para identificar a cada una de las personas del video. Una vez identificadas se les podría aplicar el seguimiento de postura para vectorizar sus movimientos. Con estas secuencias de movimeintos podría entrenarse un clasificador.  Esta propuesta tiene la ventaja de reducir el tamaño de los inputs para el entrenamiento del modelo. Sin embargo aumenta la labor de preprocesamiento de los datos. "],"metadata":{"id":"8C9nTLU6n6rJ"}}],"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNk1GGbmWIwK5Q6XwsuhXo/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}